{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Hands-on\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "Below notebook shows how to manipulate images, train/test Convolutional Neural Network and visualize the learning results on CIFAR10 dataset. More information about the dataset can be found on the [Alex Krizhevsky's page](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Before running below hands-on, recall all your knowledge about:\n",
    " - Training neural networks with SGD,\n",
    " - Convolutional Neural Networks.\n",
    "\n",
    "You can also refer to my presentation that you can find in the root directory of this repository. HTML version with all the GIFs is available [here](https://mega.nz/#%21H4IEnZKJ%21so0Czkp8lcLWCt0o3O912WnKZBFjkvZFeJG23kITpig)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you start\n",
    "\n",
    "In below code you can find many tags that highlight places on which you can work.\n",
    "\n",
    "**Available tags:**\n",
    " - `[TRY ME]` - places where you can change some values and try how such entries affect other components,\n",
    " - `[TODO]` - places where you have to write your own implementation for some functions/parts of code.\n",
    "\n",
    "Let's start :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "At first, let's prepare the dataset with all the images and classes. We'll use `torchvision` package which is great to start working with the most popular datasets with just one line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import all the packages we will use during this hands-on\n",
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our dataset. The CIFAR-10 dataset consists of **60000 32x32 colour images in 10 classes**, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset will download automatically into the root directory of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10('.', train=True, download=True)\n",
    "print('There are {} training pictures.'.format(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, all of the images have already been splitted into train and test set by the `torchvision` library. With this handy trick, we can save our time working with the original files :)\n",
    "\n",
    "Now, let's get all available labels from the metadata file stored together with the CIFAR10 images. It's a simple Python dictionary pickled into a file, so the only thing we need to do is load it and read the `label_names` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cifar-10-batches-py/batches.meta', 'rb') as metadata:\n",
    "    LABELS = pickle.load(metadata)['label_names']\n",
    "print('All available classes: {}.'.format(LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the dataset itself and visualise an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image, example_class = random.choice(train_dataset)\n",
    "print('Below image shows: {}'.format(LABELS[example_class]))\n",
    "plt.imshow(example_image)  # [TRY ME] Check interpolation methods, eg. 'gaussian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start our preprocessing with conversion of the images from the CIFAR10 datasets to the numpy arrays. Currently, they are PIL Images, which makes them unuseable with any of the available Machine Learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image, example_class = random.choice(train_dataset)\n",
    "print('Types before conversion: ({}, {})'.format(type(example_image), type(example_class)))\n",
    "\n",
    "conversion_to_numpy = lambda example: (np.array(example[0]), example[1])\n",
    "train_dataset = list(map(conversion_to_numpy, train_dataset))\n",
    "\n",
    "example_image, example_class = random.choice(train_dataset)\n",
    "print('Types after conversion:  ({}, {})'.format(type(example_image), type(example_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've prepared data for further manipulation, it's time to split our initial training dataset into train and validation sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TRY ME] Proportion in which we should split training dataset into smaller sets\n",
    "VALIDATION = 0.2\n",
    "\n",
    "# Let's compute where we should split our training dataset\n",
    "number_of_training_examples = len(train_dataset)\n",
    "indices = list(range(number_of_training_examples))\n",
    "splitting_point = int(np.floor(VALIDATION * number_of_training_examples))\n",
    "\n",
    "# Shuffle all the indices, so our dataset will be equally distributed\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split the indices in the splitting point\n",
    "train_idx, valid_idx = indices[splitting_point:], indices[:splitting_point]\n",
    "\n",
    "# Prepare training and validation datasets with examples\n",
    "training_images = [train_dataset[i][0] for i in train_idx]\n",
    "training_classes = [train_dataset[i][1] for i in train_idx]\n",
    "validation_images = [train_dataset[i][0] for i in valid_idx]\n",
    "validation_classes = [train_dataset[i][1] for i in valid_idx]\n",
    "print('Initial training dataset has: {} examples.'.format(len(train_dataset)))\n",
    "print('Now, training dataset has: {} examples.'.format(len(training_images)))\n",
    "print('Now, validation dataset has: {} examples.'.format(len(validation_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall from CS231n course\n",
    "\n",
    "![Data Preprocessing](./assets/data_preprocessing.jpeg)\n",
    "\n",
    "> **Common pitfall.** An important point to make about the preprocessing is that any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation / test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).\n",
    "http://cs231n.github.io/neural-networks-2/#datapre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute mean and standard deviation, which will be used to zero center and normalize dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std dev for all images from the training dataset\n",
    "MEAN_IMAGE = np.mean(training_images, axis=0)\n",
    "STD_DEV_IMAGE = np.std(training_images, axis=0)\n",
    "\n",
    "# Let's visualize them!\n",
    "fig, subplots = plt.subplots(1, 2)\n",
    "subplots[0].set_title('Mean')\n",
    "subplots[0].imshow(MEAN_IMAGE)\n",
    "subplots[1].set_title('Std')\n",
    "subplots[1].imshow(STD_DEV_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, we've got mean and standard derivative let's apply them to our datasets (both training and validation dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = (training_images - MEAN_IMAGE) / STD_DEV_IMAGE\n",
    "validation_images = (validation_images - MEAN_IMAGE) / STD_DEV_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one last thing is strictly connected with the way we will create our model. The standard approach (in the most frameworks/papers) is to use the \"channels first\" order, where the first dimension of the input array is the feature channel. Right now, our images have `(NUMBER_OF_IMAGES, 32, 32, 3)` shape. In order to fit them into the neural network, we've got to swap the last dimension with the second one, so our images will follow the `[BATCH, CHANNEL, IMAGE_Y, IMAGE_X]` approach.\n",
    "\n",
    "**Remember** to always work on both the training and validation dataset! Later in this notebook, we will also apply such transformations to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training examples before: {}'.format(training_images.shape))\n",
    "print('Validation examples before: {}'.format(validation_images.shape))\n",
    "\n",
    "training_images = np.swapaxes(training_images, 1, 3)\n",
    "validation_images = np.swapaxes(validation_images, 1, 3)\n",
    "\n",
    "print('Training examples after: {}'.format(training_images.shape))\n",
    "print('Validation examples after: {}'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are now ready to be used! But... our classes needs to be transformed into \"one hot\" format.\n",
    "\n",
    "**One Hot** format is a way to represent our integer classes with the binary representation. Such values will be easier to reconstruct by the neural network on the last layer.\n",
    "\n",
    "_**Example one hot mapping:**_\n",
    "\n",
    "| Class      | Previously | One Hot Representation         |\n",
    "|------------|------------|--------------------------------|\n",
    "| airplane   | 0          | [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] |\n",
    "| automobile | 1          | [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] |\n",
    "| bird       | 2          | [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] |\n",
    "| ...        | ...        | ...                            |\n",
    "\n",
    "Let's convert our classes to the One Hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(old_class):\n",
    "    \"\"\"Convert classic integer label to the 'one hot' format\"\"\"\n",
    "    # [TODO] Fill me!\n",
    "    return old_class\n",
    "\n",
    "training_classes = list(map(convert_to_one_hot, training_classes))\n",
    "validation_classes = list(map(convert_to_one_hot, validation_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training and validation examples are now ready to be used for training our Convolutional Neural Network!\n",
    "\n",
    "Yay! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare CNN model\n",
    "It's high time to prepare our Convolutional Neural Network model! We'll use PyTorch to do so :)\n",
    "\n",
    "**Why PyTorch?** Mostely, because it's great for learning! It shows all the inside things that has to happen to train our network. Every other framework will do many of these things for us but here we've got to do it on our own. What's more, PyTorch is written in pure Python, which makes it great to experiment with. It's also Open Source, so you can look inside of the code, ask people about it (community is great) and even work on your own!\n",
    "\n",
    "Let's prepare the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    \"\"\"Our Convolutional Neural Network model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the network components\"\"\"\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        # [TODO] 2DConv with 3 input channels, 32 filters output and 3x3 kernel filter\n",
    "        # [TODO] 2DConv with 32 input channels, 64 filters output and 3x3 kernel filter\n",
    "        # [TODO] 2DConv with 64 input channels, 128 filters output and 3x3 kernel filter\n",
    "        # [TODO] Dense linear layer with 512 input neurons and 128 output neurons\n",
    "        # [TODO] Dense linear layer with 128 input neurons and 128 output neurons\n",
    "        # [TODO] Dense linear layer with 128 input neurons and 10 output neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass of the network\"\"\"\n",
    "        # Current x: [BATCH_SIZE, 3, 32, 32]\n",
    "        # [TODO] First 2DConv with ReLu and 2DMaxPooling with 2x2 filters\n",
    "        \n",
    "        # Current x: [BATCH_SIZE, 32, 15, 15]\n",
    "        # [TODO] Second 2DConv with ReLu and 2DMaxPooling with 2x2 filters\n",
    "        \n",
    "        # Current x: [BATCH_SIZE, 64, 6, 6]\n",
    "        # [TODO] Third 2DConv with ReLu and 2DMaxPooling with 2x2 filters\n",
    "\n",
    "        # Current x: [BATCH_SIZE, 128, 2, 2]\n",
    "        # [TODO] Flatten x, so we'll be able to pass it into the linear layer\n",
    "\n",
    "        # Current x: [BATCH_SIZE, 512]\n",
    "        # [TODO] First linear layer with ReLu\n",
    "\n",
    "        # Current x: [BATCH_SIZE, 128]\n",
    "        # [TODO] Second linear layer with ReLu\n",
    "\n",
    "        # Current x: [BATCH_SIZE, 128]\n",
    "        # [TODO] Third linear layer with ReLu\n",
    "\n",
    "        # Current x: [BATCH_SIZE, 10]\n",
    "        return x\n",
    "\n",
    "    def get_number_of_flat_features(self, x):\n",
    "        \"\"\"Calculate number of flat features\"\"\"\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've got to create our network by calling the class' initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ConvolutionalNeuralNetwork()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Our model is ready to be trained. Before we do so, let's prepare some helper/utility functions.\n",
    "\n",
    "First one will help us with shuffling all given examples. It's very important to shuffle images and classes in the same way, so that they'll be still coupled! We don't want to loose the dataset and mix all the labels :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_examples(images, classes):\n",
    "    \"\"\"Shuffle images & classes and respects they order\"\"\"\n",
    "    combined = list(zip(images, classes))\n",
    "    random.shuffle(combined)\n",
    "    new_images, new_classes = zip(*combined)\n",
    "    return new_images, new_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the metrics that we will be using is accuracy. **Accuracy** tell us how many labels were properly classified. To do so, we'll check the best class which was predicted and true label from the dataset. If they are the same, we should increase the accuracy. Accuracy will be representend as a percentage value.\n",
    "\n",
    "That's why we need a function that tell us number of correct labels for given batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_correct_labels(network_output, true_labels):\n",
    "    \"\"\"Return number of correctly predicted labels\n",
    "    \n",
    "    Correct classification return 1 for given example.\n",
    "    \"\"\"\n",
    "    predicted_classes = network_output.topk(1, 1)[1]  # Indexes for the Top-1 values\n",
    "    true_classes = true_labels.topk(1, 1)[1]  # Indexes for the Top-1 values\n",
    "    return predicted_classes.eq(true_classes).float().sum().data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a function that will update the figure with Loss and Accuracy on both the training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_figures(figure, training_losses, validation_losses, training_accuracies, validation_accuracies):\n",
    "    \"\"\"Update and draw the figure with 'Loss' and 'Accuracy' plots\"\"\"\n",
    "    # Clear whole figure - remove all content, titles, legend, everything!\n",
    "    figure.clear()\n",
    "\n",
    "    # 'Loss' plot\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.grid(True)\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.plot(range(len(training_losses)), training_losses, '.r-', label='Training')\n",
    "    plt.plot(range(len(validation_losses)), validation_losses, '.b-', label='Validation')\n",
    "    \n",
    "    # 'Accuracy' plot\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.grid(True)\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Value [%]')\n",
    "    training, = plt.plot(range(len(training_accuracies)), training_accuracies, '.r-',\n",
    "                         label='Training')\n",
    "    validation, = plt.plot(range(len(validation_accuracies)), validation_accuracies, '.b-',\n",
    "                           label='Validation')\n",
    "    \n",
    "    # Final rendering\n",
    "    plt.tight_layout()  # Make all the above plots look neat and tidy\n",
    "    plt.legend(bbox_to_anchor=(1,0), loc='lower right', bbox_transform=figure.transFigure, ncol=3)\n",
    "    figure.canvas.draw()  # Update the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define functions that will train our Convolutional Neural Network, let's define the hyperparameters for our training. These will be:\n",
    " - `BATCH_SIZE` - tell us how many examples are in a single batch,\n",
    " - `LEARNING_RATE` - tell us how much our weights will be updated using optimizer,\n",
    " - `NUMBER_OF_EPOCHS` - tell us how long should we run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TRY ME] All hyper parameters for the training\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.1\n",
    "NUMBER_OF_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also two additional things:\n",
    " - `loss_function` - will be used to compute the loss. In our case it's Binary Cross Entropy,\n",
    " - `optimizer` - defines the opitimizer (algorithm for optimizing weights) which will be used during the training. In our case it's SGD.\n",
    "\n",
    "Feel free to play with these things and check the results of the training :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TRY ME] Prepare loss function with optimizer\n",
    "loss_function = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our training function. It will take images and classes as an input and return loss and accuracy as an output. The training will use mini-batches with the size defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_images, training_classes):\n",
    "    _epoch_losses = []  # Keep losses for each batch\n",
    "    _epoch_properly_classified = []  # Contains numbers of properly classified images per batch\n",
    "    \n",
    "    # Train our network in batches\n",
    "    NUMBER_OF_TRAINING_BATCHES = int(len(training_images) / BATCH_SIZE)\n",
    "    for batch_number in range(NUMBER_OF_TRAINING_BATCHES):\n",
    "        if batch_number % 100 == 0:\n",
    "            print('Batch #{}/{}...'.format(batch_number, NUMBER_OF_TRAINING_BATCHES))\n",
    "\n",
    "        # Take batch of images & classes and convert them to the PyTorch Variable for further use\n",
    "        batch_images = training_images[batch_number * BATCH_SIZE:(batch_number+1) * BATCH_SIZE]\n",
    "        batch_images = Variable(torch.from_numpy(np.array(batch_images)).float())\n",
    "        batch_classes = training_classes[batch_number * BATCH_SIZE:(batch_number+1) * BATCH_SIZE]\n",
    "        batch_classes = Variable(torch.from_numpy(np.array(batch_classes)).float())\n",
    "        \n",
    "        # Let's train the network!\n",
    "        # [TODO] Reset all gradients in the model\n",
    "        # [TODO] Compute output based on input images\n",
    "        # [TODO] Compute loss based on output and true classes\n",
    "        # [TODO] Compute gradients needed to tune the network's weights\n",
    "        # [TODO] Backprop with above gradients\n",
    "\n",
    "        # Remember metrics for this batch\n",
    "        _epoch_losses.append(loss.data[0])\n",
    "        _epoch_properly_classified.append(get_number_of_correct_labels(net_output, batch_classes))\n",
    "\n",
    "    # Loss for this epoch is equal to the mean of all the losses collected for each batch\n",
    "    training_loss = np.mean(np.array(_epoch_losses))\n",
    "\n",
    "    # Accuracy for this epoch is equal to all the correctly classified images\n",
    "    # divided by all of the training examples\n",
    "    training_accuracy = 100. * np.sum(_epoch_properly_classified) / len(training_images)\n",
    "\n",
    "    return training_loss, training_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation looks (nearly) the same. And... it acctually is a copy-paste :) The only thing that has changed are not computing the gradients and not updating weights with backpopagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(validation_images, validation_classes):\n",
    "    _epoch_losses = []  # Keep losses for each batch\n",
    "    _epoch_properly_classified = []  # Contains numbers of properly classified images per batch\n",
    "    \n",
    "    # Validate our network in batches\n",
    "    NUMBER_OF_VALIDATION_BATCHES = int(len(validation_images) / BATCH_SIZE)\n",
    "    for batch_number in range(NUMBER_OF_VALIDATION_BATCHES):\n",
    "        if batch_number % 100 == 0:\n",
    "            print('Batch #{}/{}...'.format(batch_number, NUMBER_OF_VALIDATION_BATCHES))\n",
    "\n",
    "        # Take batch of images & classes and convert them to the PyTorch Variable for further use\n",
    "        batch_images = validation_images[batch_number * BATCH_SIZE:(batch_number+1) * BATCH_SIZE]\n",
    "        batch_images = Variable(torch.from_numpy(np.array(batch_images)).float())\n",
    "        batch_classes = validation_classes[batch_number * BATCH_SIZE:(batch_number+1) * BATCH_SIZE]\n",
    "        batch_classes = Variable(torch.from_numpy(np.array(batch_classes)).float())\n",
    "\n",
    "        # Let's validate the network!\n",
    "        # [TODO] Compute output based on input images\n",
    "        # [TODO] Compute loss based on output and true classes\n",
    "        \n",
    "        # Remember metrics for this batch\n",
    "        _epoch_losses.append(loss.data[0])\n",
    "        _epoch_properly_classified.append(get_number_of_correct_labels(net_output, batch_classes))\n",
    "\n",
    "    # Loss for this epoch is equal to the mean of all the losses collected for each batch\n",
    "    validation_loss = np.mean(np.array(_epoch_losses))\n",
    "\n",
    "    # Accuracy for this epoch is equal to all the correctly classified images\n",
    "    # divided by all of the validation examples\n",
    "    validation_accuracy = 100. * np.sum(_epoch_properly_classified) / len(validation_images)\n",
    "    \n",
    "    return validation_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our plots needs to store the history somewhere, so let's define places for them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear history of the training losses and accuracies\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare figure to show losses and accuracy\n",
    "plt.close()\n",
    "figure = plt.figure()\n",
    "update_figures(figure, training_losses, validation_losses,\n",
    "               training_accuracies, validation_accuracies)\n",
    "\n",
    "# Train the network in epochs\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    print('Starting epoch #{}.'.format(epoch))\n",
    "    \n",
    "    # Let's shuffle all the training & validation examples\n",
    "    training_images, training_classes = shuffle_examples(training_images, training_classes)\n",
    "    validation_images, validation_classes = shuffle_examples(validation_images, validation_classes)\n",
    "\n",
    "    # Train our network\n",
    "    training_loss, training_accuracy = train(training_images, training_classes)\n",
    "    training_losses.append(training_loss)  # History for 'Loss' plot\n",
    "    training_accuracies.append(training_accuracy)  # History for 'Accuracy' plot\n",
    "    \n",
    "    # Debug logging and update the figures\n",
    "    print(' Training loss: {:.4f}.'.format(training_loss))\n",
    "    print(' Training accuracy: {:.2f}%'.format(training_accuracy))\n",
    "    update_figures(figure, training_losses, validation_losses,\n",
    "                   training_accuracies, validation_accuracies)\n",
    "\n",
    "    # Validate our network\n",
    "    validation_loss, validation_accuracy = validation(validation_images, validation_classes)\n",
    "    validation_losses.append(validation_loss)  # History for 'Loss' plot\n",
    "    validation_accuracies.append(validation_accuracy)  # History for 'Accuracy' plot\n",
    "\n",
    "    # Debug logging and update the figures\n",
    "    print(' Validation loss: {:.4f}.'.format(validation_loss))\n",
    "    print(' Validation accuracy: {:.2f}%'.format(validation_accuracy))\n",
    "    update_figures(figure, training_losses, validation_losses,\n",
    "                   training_accuracies, validation_accuracies)\n",
    "    \n",
    "    # [TRY ME] Here, you can add some additional manipulation on optimizer based on\n",
    "    #          training & validation metrics, eg. lower the Learning Rate if validation loss\n",
    "    #          is greater than training loss (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always save the current weights with below method that will store the current state of the network on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_filename = 'model.pt'\n",
    "# torch.save(cnn_net.state_dict(), os.getcwd() + '/' + model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar way you can always restore the state of the network with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_filename = 'model.pt'\n",
    "# cnn_net = ConvolutionalNeuralNetwork()\n",
    "# cnn_net.load_state_dict(torch.load(os.getcwd() + '/' + model_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our solution\n",
    "To test our solution we will use the test dataset delivered with CIFAR10 itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CIFAR10('.', train=False)\n",
    "print('There are {} test pictures.'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test our neural network properly, we've got to prepare our images in the same way we've done it with the validation dataset. We'll use **the same** mean and standard deviation values as we've used previously. We'll also do **the same** transformations as on train/validation dataset (very important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all images to numpy arrays\n",
    "test_dataset = list(map(conversion_to_numpy, test_dataset))\n",
    "\n",
    "# Split the test dataset into images and classes\n",
    "test_images = np.array([example[0] for example in test_dataset])\n",
    "test_classes = np.array([example[1] for example in test_dataset])\n",
    "\n",
    "# Apply **the same** mean and std values to the test examples\n",
    "test_images = (test_images - MEAN_IMAGE) / STD_DEV_IMAGE\n",
    "\n",
    "# Swap the channels to match the network input ([SIZE, 32, 32, 3] -> [SIZE, 3, 32, 32])\n",
    "test_images = np.swapaxes(test_images, 1, 3)\n",
    "\n",
    "# Convert all classes to \"One Hot\" format\n",
    "test_classes = np.array(list(map(convert_to_one_hot, test_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we'll get the loss and accuracy using validation method. We can do so, because it doesn't do anything more than we want now :) In the future it may happen that validation method may do something more, so be aware about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = validation(test_images, test_classes)\n",
    "print('Test loss: {:.4f}.'.format(test_loss))\n",
    "print('Test accuracy: {:.2f}%'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is performing somehow. It's not bad but it's also not perfect...\n",
    "\n",
    "Even though, let's visualise some examples and verify the predicted classes on our own :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close previously opened plot - needed due to interrupting drawing loop of the previous figure\n",
    "plt.close()\n",
    "\n",
    "# Choose random image from the test dataset and prepare input/output for the network\n",
    "index = random.choice(range(len(test_dataset)))\n",
    "test_image, proper_class = test_images[index], test_classes[index]\n",
    "\n",
    "# Input image has to be expanded with the batch dimension ([3, 32, 32] -> [1, 3, 32, 32])\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Predict class for above random image\n",
    "test_image = Variable(torch.from_numpy(test_image).float())  # All the network inputs has to be PyTorch's Variables!\n",
    "predicted_class = cnn(test_image)\n",
    "\n",
    "# Let's get classes based on \"One Hot\" format (which means that we are looking for the\n",
    "# index/argument with the maximum value)\n",
    "predicted_class = predicted_class.data.numpy()\n",
    "predicted_class = np.argmax(predicted_class)\n",
    "proper_class = np.argmax(proper_class)\n",
    "\n",
    "# Show the image with true/predicted classes\n",
    "print('Below image shows: {}'.format(LABELS[proper_class]))\n",
    "print('Our network predicted: {}'.format(LABELS[predicted_class]))\n",
    "plt.imshow(test_dataset[index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "That's all for this hands-on! I hope you've learnt something useful :)\n",
    "\n",
    "As you could see above, our model is not perfect. It now your job to make it better! Use some other architectures and try to train them on your own!\n",
    "\n",
    "In case you've got any question related with the above notebook - send me an email (you can find it on my [GitHub profile](http://github.com/jpowie01/)).\n",
    "\n",
    "**Remember to star this repository if you like it :)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
